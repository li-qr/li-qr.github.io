---
title: Kafka权威指南摘要
categories:
- ZooKeeper
- JAVA
- 分布式
description: Kafka权威指南摘要
permalink: "/posts/kafka-the-definitive-guide"
excerpt: 内容全部源自《Kafka权威指南》，取自里面的重点内容摘要。包括为什么选择kafka、控制器、存储等。
---

# 初识Kafka

## 发布与订阅消息系统

数据（消息）的发送者不会直接把消息发送给接收者。发布者以某种方式对消息进行分类，接收者订阅他们以便接收特定类型的消息。发布与订阅系统一般会有一个broker，也就是发布消息的中心点。

如果每个发布者都直接与订阅者通信，当项目复杂之后将形成一张庞大的不可维护的通信网络。这时就需要独立的队列系统。

### 主题和分区

Kafka 的消息通过主题进⾏分类。主题可以被分 为若⼲个分区，⼀个分区就是⼀个提交⽇志。消息以追加的⽅式写⼊分区。⽆法在整个主题范围内保证消息的顺序，但可以保证消息在单个分区内的顺序。Kafka 通过分区来实现数据冗余和伸缩性。分区可以分布在不同的服务器上，也就是说，⼀个主题可以横跨多个服务器，以此来提供⽐单个服务器更强⼤的性能。

![包含多个分区的主题表⽰](/assets/images/b7022229-6a90-4d5a-8a99-551634c32e50.png)

### 生产者和消费者

Kafka 的客户端就是 Kafka 系统的⽤户，它们被分为两种基本类型：⽣产者和消费者。除此之外，还有其他⾼级客户端 API——⽤于数据集成的 Kafka Connect API 和⽤于流式处理的 Kafka Streams。

⽣产者创建消息。⽣产者在默认情况下把消息均衡地分布到主题的所有分区上，在某些情况下，⽣产者会把消息直接写到指定的分区。这通常是通过消息键和分区器来实现的，分区器为键⽣成⼀个散列值，并将其映射到指定的分区上。这样可以保证包含同⼀个键的消息会被写到同⼀个分区上。⽣产者也可以使⽤⾃定义的分区器，根据不同的业务规则将消息映射到分区。

消费者读取消息。消费者订阅⼀个或多个主题，并按照消息⽣成的顺序读取它们。消费者通过检查消息的偏移量来区分已经读取过的消息。偏移量是另⼀种元数据，它是⼀个不断递增的整数值，在创建消息时，Kafka会把它添加到消息⾥。在给定的分区⾥，每个消息的偏移量都是唯⼀的。消费者把每个分区最后读取的消息偏移量保存在 Zookeeper 或 Kafka 上，如果消费者关闭或重启，它的读取状态不会丢失。

消费者是消费者群组的⼀部分，也就是说，会有⼀个或多个消费者共同读取⼀个主题。群组保证每个 分区只能被⼀个消费者使⽤。消费者与分区之间的映射通常被称为消费者对分区的所有权关系。

![消费者群组从主题读取消息](/assets/images/761ba8b8-440a-4837-8c9a-48d024543f93.png)

### broker和集群

⼀个独⽴的 Kafka 服务器被称为 broker。broker 接收来⾃⽣产者的消息，为消息设置偏移量，并提交消息到磁盘保存。broker 为消费者提供服务，对读取分区的请求作出响应，返回已经提交到磁盘上的消息。

每个集群都有⼀个 broker 同时充当了集群控制器的⾓⾊（⾃动从集群的活跃成员中选举出来）。控制器负责管理⼯作，包括将分区分配给 broker 和监控 broker。在集群中，⼀个分区从属于⼀个 broker，该 broker 被称为分区的⾸领。⼀个分区可以分配给多个 broker， 这个时候会发⽣分区复制。这种复制机制为分区提供了消息冗余，如果有⼀个 broker 失效，其他 broker 可以接管领导权。不过，相关的消费者和⽣产者都要重新连接到新的⾸领。

![集群里的分区复制](/assets/images/865cd464-1e80-4ccb-a94c-7eb4440d5dd6.png)

Kafka broker 默认的消息过期删除策略可以按时间或大小。Kafka的多集群复制不完善。

## 为什么选择Kafka

1. 可以支持多个生产者
2. 支持多个消费者且互不影响
3. 基于磁盘的数据存储，允许消息回溯和积压
4. 灵活的可伸缩性
5. 通过横向扩展拥有高性能

# Kafka配置

## broker

### broker.id

每个 broker 都需要有⼀个标识符，使⽤ broker.id 来表⽰。它的默认值是 0，也可以被设置成其他任意整数。这个值在整个 Kafka 集群⾥必须是唯⼀的。

### port

默认9092

###  zookeeper.connect

⽤于保存 broker 元数据的 Zookeeper 地址是通过 zookeeper.connect 来指定的。

### log.dirs

Kafka 把所有消息都保存在磁盘上，存放这些⽇志⽚段的⽬录是通过 log.dirs 指定的。它是⼀组⽤逗号分隔的本地⽂件系统路径。如果指定了多个路径，那么 broker 会根据“最少使⽤”原则， 把同⼀个分区的⽇志⽚段保存到同⼀个路径下。要注意，broker 会往拥有最少数⽬分区的路径新增分区，⽽不是往拥有最⼩磁盘空间的路径新增分区。

### num.recovery.threads.per.data.dir

对于如下 3 种情况，Kafka 会使⽤可配置的线程池来处理⽇志⽚段： 

+ 服务器正常启动，⽤于打开每个分区的⽇志⽚段； 
+ 服务器崩溃后重启，⽤于检查和截短每个分区的⽇志⽚段； 
+ 服务器正常关闭，⽤于关闭⽇志⽚段。 

默认情况下，每个⽇志⽬录只使⽤⼀个线程。因为这些线程只是在服务器启动和关闭时会⽤到， 所以完全可以设置⼤量的线程来达到并⾏操作的⽬的。特别是对于包含⼤量分区的服务器来说， ⼀旦发⽣崩溃，在进⾏恢复时使⽤并⾏操作可能会省下数⼩时的时间。设置此参数时需要注意， 所配置的数字对应的是 log.dirs 指定的单个⽇志⽬录。也就是说，如果
num.recovery.threads.per.data.dir 被设为 8，并且 log.dir 指定了 3 个路径，那么 总共需要 24 个线程。

### auto.create.topics.enable

默认情况下，Kafka 会在如下⼏种情形下⾃动创建主题： 

+ 当⼀个⽣产者开始往主题写⼊消息时； 
+ 当⼀个消费者开始从主题读取消息时； 
+ 当任意⼀个客户端向主题发送元数据请求时。 

很多时候，这些⾏为都是⾮预期的。⽽且，根据 Kafka 协议，如果⼀个主题不先被创建，根本⽆法知道它是否已经存在。如果显式地创建主题，不管是⼿动创建还是通过其他配置系统来创建， 都可以把 auto.create.topics.enable 设为 false。

### num.partitions

num.partitions 参数指定了新创建的主题将包含多少个分区。如果启⽤了主题⾃动创建功能，主题分区的个数就是该参数指定的值。该参数的默认值是 1。要注意， 我们可以增加主题分区的个数，但不能减少分区的个数。所以，如果要让⼀个主题的分区个数少于 num.partitions 指定的值，需要⼿动创建该主题。

### log.retention.ms

Kafka 通常根据时间来决定数据可以被保留多久。默认使⽤ log.retention.hours 参数来配置时间，默认值为 168 ⼩时，也就是⼀周。除此以外，还有其他两个参数 log.retention.minutes 和 log.retention.ms。这 3 个参数的作⽤是⼀样的，都是决定消息多久以后会被删除，不过还是推荐使⽤ log.retention.ms。如果指定了不⽌⼀个参数， Kafka 会优先使⽤具有最⼩值的那个参数。

根据时间保留数据是通过检查磁盘上⽇志⽚段⽂件的最后修改时间来实现的。⼀般来说，最 后修改时间指的就是⽇志⽚段的关闭时间，也就是⽂件⾥最后⼀个消息的时间戳。不过，如 果使⽤管理⼯具在服务器间移动分区，最后修改时间就不准确了。时间误差可能导致这些分 区过多地保留数据。

### log.retention.bytes

另⼀种⽅式是通过保留的消息字节数来判断消息是否过期。它的值通过参数 log.retention.bytes 来指定，作⽤在每⼀个分区上。也就是说，如果有⼀个包含 8 个分区 的主题，并且 log.retention.bytes 被设为 1GB，那么这个主题最多可以保留 8GB 的数 据。所以，当主题的分区个数增加时，整个主题可以保留的数据也随之增加。

如果同时指定了 log.retention.bytes 和 log.retention.ms（或者另⼀个时间参数），只要任意⼀个条件得到满⾜，消息就会被删除。

### log.segment.bytes

以上的设置都作⽤在⽇志⽚段上，⽽不是作⽤在单个消息上。当消息到达 broker 时，它们被追加到分区的当前⽇志⽚段上。当⽇志⽚段⼤⼩达到 log.segment.bytes 指定的上限（默认是 1GB）时，当前⽇志⽚段就会被关闭，⼀个新的⽇志⽚段被打开。如果⼀个⽇志⽚段被关闭，就 开始等待过期。这个参数的值越⼩，就会越频繁地关闭和分配新⽂件，从⽽降低磁盘写⼊的整体效率。

如果⼀个主题每天只接收 100MB 的消息，⽽ log.segment.bytes 使⽤默认设置，那么需要 10 天时间才能填满⼀个⽇志⽚段。因为在⽇志⽚段被关闭之前消息是不会过期的，所以如果过期时间设置为7天，那么⽇志⽚段最多需要 17 天才 会过期。 这是因为关闭⽇志⽚段需要 10 天的时间，⽽根据配置的过期时间，还需要再保留 7 天时间（要 等到⽇志⽚段⾥的最后⼀个消息过期才能被删除）。

在使⽤时间戳获取⽇志偏移量时，Kafka 会检查分区⾥最后修改时间⼤于指定时间戳的⽇志⽚段（已经被关闭的），该⽇志⽚段的前⼀个⽂件的最后修改时间⼩于指定时间戳。然后，Kafka 返回该⽇志⽚段（也就是⽂件名）开头的偏移量。对于使⽤时间戳获取偏移量的操作来说，⽇志⽚段越⼩，结果越准确。

### log.segment.ms

另⼀个可以控制⽇志⽚段关闭时间的参数是 log.segment.ms，它指定了多⻓时间之后⽇志⽚段会被关闭。

### message.max.bytes

broker 通过设置 message.max.bytes 参数来限制单个消息的⼤⼩，默认值是 1 000 000，也就是1MB。如果⽣产者尝试发送的消息超过这个⼤⼩，不仅消息不会被接收，还会收到 broker 返回的错误信息。

消费者客户端设置的 fetch.message.max.bytes 必须与服务器端设置的消息⼤⼩进⾏协 调。如果这个值⽐ message.max.bytes ⼩，那么消费者就⽆法读取⽐较⼤的消息，导致出现消费者被阻塞的情况。在为集群⾥的 broker 配置 replica.fetch.max.bytes 参数时，也遵循同样的原则。