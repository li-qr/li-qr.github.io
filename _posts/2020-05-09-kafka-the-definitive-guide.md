---
title: Kafka权威指南摘要
categories:
- ZooKeeper
- JAVA
- 分布式
description: Kafka权威指南摘要
permalink: "/posts/kafka-the-definitive-guide"
excerpt: 内容全部源自《Kafka权威指南》，取自里面的重点内容摘要。包括为什么选择kafka、控制器、存储等。
---

# 初识Kafka

## 发布与订阅消息系统

数据（消息）的发送者不会直接把消息发送给接收者。发布者以某种方式对消息进行分类，接收者订阅他们以便接收特定类型的消息。发布与订阅系统一般会有一个broker，也就是发布消息的中心点。

如果每个发布者都直接与订阅者通信，当项目复杂之后将形成一张庞大的不可维护的通信网络。这时就需要独立的队列系统。

### 主题和分区

Kafka 的消息通过主题进⾏分类。主题可以被分 为若⼲个分区，⼀个分区就是⼀个提交⽇志。消息以追加的⽅式写⼊分区。⽆法在整个主题范围内保证消息的顺序，但可以保证消息在单个分区内的顺序。Kafka 通过分区来实现数据冗余和伸缩性。分区可以分布在不同的服务器上，也就是说，⼀个主题可以横跨多个服务器，以此来提供⽐单个服务器更强⼤的性能。

![包含多个分区的主题表⽰](/assets/images/b7022229-6a90-4d5a-8a99-551634c32e50.png)

### 生产者和消费者

Kafka 的客户端就是 Kafka 系统的⽤户，它们被分为两种基本类型：⽣产者和消费者。除此之外，还有其他⾼级客户端 API——⽤于数据集成的 Kafka Connect API 和⽤于流式处理的 Kafka Streams。

⽣产者创建消息。⽣产者在默认情况下把消息均衡地分布到主题的所有分区上，在某些情况下，⽣产者会把消息直接写到指定的分区。这通常是通过消息键和分区器来实现的，分区器为键⽣成⼀个散列值，并将其映射到指定的分区上。这样可以保证包含同⼀个键的消息会被写到同⼀个分区上。⽣产者也可以使⽤⾃定义的分区器，根据不同的业务规则将消息映射到分区。

消费者读取消息。消费者订阅⼀个或多个主题，并按照消息⽣成的顺序读取它们。消费者通过检查消息的偏移量来区分已经读取过的消息。偏移量是另⼀种元数据，它是⼀个不断递增的整数值，在创建消息时，Kafka会把它添加到消息⾥。在给定的分区⾥，每个消息的偏移量都是唯⼀的。消费者把每个分区最后读取的消息偏移量保存在 Zookeeper 或 Kafka 上，如果消费者关闭或重启，它的读取状态不会丢失。

消费者是消费者群组的⼀部分，也就是说，会有⼀个或多个消费者共同读取⼀个主题。群组保证每个 分区只能被⼀个消费者使⽤。消费者与分区之间的映射通常被称为消费者对分区的所有权关系。

![消费者群组从主题读取消息](/assets/images/761ba8b8-440a-4837-8c9a-48d024543f93.png)

### broker和集群

⼀个独⽴的 Kafka 服务器被称为 broker。broker 接收来⾃⽣产者的消息，为消息设置偏移量，并提交消息到磁盘保存。broker 为消费者提供服务，对读取分区的请求作出响应，返回已经提交到磁盘上的消息。

每个集群都有⼀个 broker 同时充当了集群控制器的⾓⾊（⾃动从集群的活跃成员中选举出来）。控制器负责管理⼯作，包括将分区分配给 broker 和监控 broker。在集群中，⼀个分区从属于⼀个 broker，该 broker 被称为分区的⾸领。⼀个分区可以分配给多个 broker， 这个时候会发⽣分区复制。这种复制机制为分区提供了消息冗余，如果有⼀个 broker 失效，其他 broker 可以接管领导权。不过，相关的消费者和⽣产者都要重新连接到新的⾸领。

![集群里的分区复制](/assets/images/865cd464-1e80-4ccb-a94c-7eb4440d5dd6.png)

Kafka broker 默认的消息过期删除策略可以按时间或大小。Kafka的多集群复制不完善。

## 为什么选择Kafka

1. 可以支持多个生产者
2. 支持多个消费者且互不影响
3. 基于磁盘的数据存储，允许消息回溯和积压
4. 灵活的可伸缩性
5. 通过横向扩展拥有高性能

# Kafka配置

## broker

### broker.id

每个 broker 都需要有⼀个标识符，使⽤ broker.id 来表⽰。它的默认值是 0，也可以被设置成其他任意整数。这个值在整个 Kafka 集群⾥必须是唯⼀的。

### port

默认9092

###  zookeeper.connect

⽤于保存 broker 元数据的 Zookeeper 地址是通过 zookeeper.connect 来指定的。

### log.dirs

Kafka 把所有消息都保存在磁盘上，存放这些⽇志⽚段的⽬录是通过 log.dirs 指定的。它是⼀组⽤逗号分隔的本地⽂件系统路径。如果指定了多个路径，那么 broker 会根据“最少使⽤”原则， 把同⼀个分区的⽇志⽚段保存到同⼀个路径下。要注意，broker 会往拥有最少数⽬分区的路径新增分区，⽽不是往拥有最⼩磁盘空间的路径新增分区。

### num.recovery.threads.per.data.dir

对于如下 3 种情况，Kafka 会使⽤可配置的线程池来处理⽇志⽚段： 

+ 服务器正常启动，⽤于打开每个分区的⽇志⽚段； 
+ 服务器崩溃后重启，⽤于检查和截短每个分区的⽇志⽚段； 
+ 服务器正常关闭，⽤于关闭⽇志⽚段。 

默认情况下，每个⽇志⽬录只使⽤⼀个线程。因为这些线程只是在服务器启动和关闭时会⽤到， 所以完全可以设置⼤量的线程来达到并⾏操作的⽬的。特别是对于包含⼤量分区的服务器来说， ⼀旦发⽣崩溃，在进⾏恢复时使⽤并⾏操作可能会省下数⼩时的时间。设置此参数时需要注意， 所配置的数字对应的是 log.dirs 指定的单个⽇志⽬录。也就是说，如果
num.recovery.threads.per.data.dir 被设为 8，并且 log.dir 指定了 3 个路径，那么 总共需要 24 个线程。

### auto.create.topics.enable

默认情况下，Kafka 会在如下⼏种情形下⾃动创建主题： 

+ 当⼀个⽣产者开始往主题写⼊消息时； 
+ 当⼀个消费者开始从主题读取消息时； 
+ 当任意⼀个客户端向主题发送元数据请求时。 

很多时候，这些⾏为都是⾮预期的。⽽且，根据 Kafka 协议，如果⼀个主题不先被创建，根本⽆法知道它是否已经存在。如果显式地创建主题，不管是⼿动创建还是通过其他配置系统来创建， 都可以把 auto.create.topics.enable 设为 false。

### num.partitions

num.partitions 参数指定了新创建的主题将包含多少个分区。如果启⽤了主题⾃动创建功能，主题分区的个数就是该参数指定的值。该参数的默认值是 1。要注意， 我们可以增加主题分区的个数，但不能减少分区的个数。所以，如果要让⼀个主题的分区个数少于 num.partitions 指定的值，需要⼿动创建该主题。

### log.retention.ms

Kafka 通常根据时间来决定数据可以被保留多久。默认使⽤ log.retention.hours 参数来配置时间，默认值为 168 ⼩时，也就是⼀周。除此以外，还有其他两个参数 log.retention.minutes 和 log.retention.ms。这 3 个参数的作⽤是⼀样的，都是决定消息多久以后会被删除，不过还是推荐使⽤ log.retention.ms。如果指定了不⽌⼀个参数， Kafka 会优先使⽤具有最⼩值的那个参数。

根据时间保留数据是通过检查磁盘上⽇志⽚段⽂件的最后修改时间来实现的。⼀般来说，最 后修改时间指的就是⽇志⽚段的关闭时间，也就是⽂件⾥最后⼀个消息的时间戳。不过，如 果使⽤管理⼯具在服务器间移动分区，最后修改时间就不准确了。时间误差可能导致这些分 区过多地保留数据。

### log.retention.bytes

另⼀种⽅式是通过保留的消息字节数来判断消息是否过期。它的值通过参数 log.retention.bytes 来指定，作⽤在每⼀个分区上。也就是说，如果有⼀个包含 8 个分区 的主题，并且 log.retention.bytes 被设为 1GB，那么这个主题最多可以保留 8GB 的数 据。所以，当主题的分区个数增加时，整个主题可以保留的数据也随之增加。

如果同时指定了 log.retention.bytes 和 log.retention.ms（或者另⼀个时间参数），只要任意⼀个条件得到满⾜，消息就会被删除。

### log.segment.bytes

以上的设置都作⽤在⽇志⽚段上，⽽不是作⽤在单个消息上。当消息到达 broker 时，它们被追加到分区的当前⽇志⽚段上。当⽇志⽚段⼤⼩达到 log.segment.bytes 指定的上限（默认是 1GB）时，当前⽇志⽚段就会被关闭，⼀个新的⽇志⽚段被打开。如果⼀个⽇志⽚段被关闭，就 开始等待过期。这个参数的值越⼩，就会越频繁地关闭和分配新⽂件，从⽽降低磁盘写⼊的整体效率。

如果⼀个主题每天只接收 100MB 的消息，⽽ log.segment.bytes 使⽤默认设置，那么需要 10 天时间才能填满⼀个⽇志⽚段。因为在⽇志⽚段被关闭之前消息是不会过期的，所以如果过期时间设置为7天，那么⽇志⽚段最多需要 17 天才 会过期。 这是因为关闭⽇志⽚段需要 10 天的时间，⽽根据配置的过期时间，还需要再保留 7 天时间（要 等到⽇志⽚段⾥的最后⼀个消息过期才能被删除）。

在使⽤时间戳获取⽇志偏移量时，Kafka 会检查分区⾥最后修改时间⼤于指定时间戳的⽇志⽚段（已经被关闭的），该⽇志⽚段的前⼀个⽂件的最后修改时间⼩于指定时间戳。然后，Kafka 返回该⽇志⽚段（也就是⽂件名）开头的偏移量。对于使⽤时间戳获取偏移量的操作来说，⽇志⽚段越⼩，结果越准确。

### log.segment.ms

另⼀个可以控制⽇志⽚段关闭时间的参数是 log.segment.ms，它指定了多⻓时间之后⽇志⽚段会被关闭。

### message.max.bytes

broker 通过设置 message.max.bytes 参数来限制单个消息的⼤⼩，默认值是 1 000 000，也就是1MB。如果⽣产者尝试发送的消息超过这个⼤⼩，不仅消息不会被接收，还会收到 broker 返回的错误信息。

消费者客户端设置的 fetch.message.max.bytes 必须与服务器端设置的消息⼤⼩进⾏协 调。如果这个值⽐ message.max.bytes ⼩，那么消费者就⽆法读取⽐较⼤的消息，导致出现消费者被阻塞的情况。在为集群⾥的 broker 配置 replica.fetch.max.bytes 参数时，也遵循同样的原则。

## Kafka集群

![⼀个简单的 Kafka 集群](/assets/images/cbd05564-f9f2-4517-8473-edc78b47c811.png)

# 生产者

## 概览

![Kafka生产者组件图](/assets/images/1dc4b5db-7a4d-4ab5-9088-b6e25e310173.png)

我们从创建⼀个 ProducerRecord 对象开始，ProducerRecord 对象需要包含⽬标主题和要发送的内容。我们还可以指定键或分区。在发送 ProducerRecord 对象时，⽣产者要先把键和值对象序列化 成字节数组，这样它们才能够在⽹络上传输。 

接下来，数据被传给分区器。如果之前在 ProducerRecord 对象⾥指定了分区，那么分区器就不会再做任何事情，直接把指定的分区返回。如果没有指定分区，那么分区器会根据 ProducerRecord 对象的键来选择⼀个分区。选好分区以后，⽣产者就知道该往哪个主题和分区发送这条记录了。紧接着， 这条记录被添加到⼀个记录批次⾥，这个批次⾥的所有消息会被发送到相同的主题和分区上。有⼀个
独⽴的线程负责把这些记录批次发送到相应的 broker 上。 

服务器在收到这些消息时会返回⼀个响应。如果消息成功写⼊ Kafka，就返回⼀个 RecordMetaData 对象，它包含了主题和分区信息，以及记录在分区⾥的偏移量。如果写⼊失败， 则会返回⼀个错误。⽣产者在收到错误之后会尝试重新发送消息，⼏次之后如果还是失败，就返回错 误信息。

## 创建生产者

### bootstrap.servers

该属性指定 broker 的地址清单，地址的格式为 host:port。清单⾥不需要包含所有的 broker 地址，⽣产者会从给定的 broker ⾥查找到其他 broker 的信息。

### key.serializer

broker 希望接收到的消息的键和值都是字节数组。⽣产者接⼝允许使⽤参数化类型，因此可以把 Java 对象作为键和值发送给 broker。key.serializer 必须被设置为⼀个实现了 org.apache.kafka.common.serialization.Serializer 接⼝的类，⽣产者会使⽤这个类把键对象序列化成字节数组。Kafka 客户端默认提供了 ByteArraySerializer、StringSerializer 和 IntegerSerializer，因此，如果你只使⽤常⻅的⼏种 Java 对象类型，那么就没必要实现⾃⼰的序列化器。要注意，key.serializer 是必须设置的，就算你打算只发送值内容。

### value.serializer

与 key.serializer ⼀样，value.serializer 指定的类会将值序列化。如果键和值都是字 符串，可以使⽤与 key.serializer ⼀样的序列化器。如果键是整数类型⽽值是字符串，那么需要 使⽤不同的序列化器。

### acks

acks 参数指定了必须要有多少个分区副本收到消息，⽣产者才会认为消息写⼊是成功的。

如果 acks=0，⽣产者在成功写⼊消息之前不会等待任何来⾃服务器的响应。 

如果 acks=1，只要集群的⾸领节点收到消息，⽣产者就会收到⼀个来⾃服务器的成功响应。如果消息⽆法到达⾸领节点，⽣产者会收到⼀个错误响应。

如果 acks=all，只有当所有参与复制的节点全部收到消息时，⽣产者才会收到⼀个来⾃服务器的成功响应。

###  buffer.memory

该参数⽤来设置⽣产者内存缓冲区的⼤⼩，⽣产者⽤它缓冲要发送到服务器的消息。如果应⽤程序发送消息的速度超过发送到服务器的速度，会导致⽣产者空间不⾜。这个时候， send() ⽅法 调⽤要么被阻塞，要么抛出异常，取决于如何设置 block.on.buffer.full 参数（在 0.9.0.0 版本⾥被替换成了 max.block.ms，表⽰在抛出异常之前可以阻塞⼀段时间）。

### compression.type

默认情况下，消息发送时不会被压缩。该参数可以设置为 snappy、gzip 或 lz4，它指定了消息 被发送给 broker 之前使⽤哪⼀种压缩算法进⾏压缩。

### retries

retries 参数的值决定了⽣产者可以重发消息的次数，如果达到这个次数，⽣产者会放弃重试并返回错误。默认情况下，⽣产者会在每次重试之间等待 100ms，不过可以通过
retry.backoff.ms 参数来改变这个时间间隔。

### batch.size

当有多个消息需要被发送到同⼀个分区时，⽣产者会把它们放在同⼀个批次⾥。该参数指定了⼀个批次可以使⽤的内存⼤⼩，按照字节数计算。当批次被填满，批次⾥的所有消息会被发送出去。不过⽣产者并不⼀定都会等到批次被填满才发送，半满的批次，甚⾄只包含⼀个消息的批次也有可能被发送。所以就算把批次⼤⼩设置得很⼤，也不会造成延迟，只是会占⽤更多的内存⽽已。但如果设置得太⼩，因为⽣产者需要更频繁地发送消息，会增加⼀些额外的开销。

### 发送

发送消息主要有以下 3 种⽅式。 

#### 发送并忘记（fire-and-forget） 

我们把消息发送给服务器，但并不关⼼它是否正常到达。⼤多数情况下，消息会正常到达，因为 Kafka 是⾼可⽤的，⽽且⽣产者会⾃动尝试重发。不过，使⽤这种⽅式有时候也会丢失⼀些消息。 

#### 同步发送

我们使⽤ send() ⽅法发送消息，它会返回⼀个 Future 对象，调⽤ get() ⽅法进⾏等待，就可以知道消息是否发送成功。 

#### 异步发送

我们调⽤ send() ⽅法，并指定⼀个回调函数，服务器在返回响应时调⽤该函数。 在下⾯的⼏个例⼦中，我们会介绍如何使⽤上述⼏种⽅式来发送消息，以及如何处理可能发⽣的异常情况。

